# Offline AI Assistant ğŸ¤–

An **offline AI assistant** built using **FastAPI**, **Ollama (local LLM)**, and **Flutter Web**.  
This project allows users to chat with an AI model **completely offline**, without relying on any cloud APIs.

---

## ğŸš€ Features

- âœ… Fully **offline AI chat**
- âœ… Uses **Ollama** for local LLM inference
- âœ… **FastAPI** backend for API handling
- âœ… **Flutter Web** frontend (works without OpenGL)
- âœ… Simple REST API (`/chat`)
- âœ… Clean and modular architecture
- âœ… Cross-platform (runs on Linux, browser-based UI)

---

## ğŸ—ï¸ Architecture

Browser (Flutter Web UI)
|
| HTTP POST
v
FastAPI Backend (localhost:8000)
|
v
Ollama (Local LLM - TinyLlama)

## ğŸ“ Project Structure
offline_ai_assistant/
â”œâ”€â”€ backend/ # FastAPI backend
â”‚ â”œâ”€â”€ main.py
â”‚ â””â”€â”€ venv/
â”œâ”€â”€ frontend/ # Flutter Web UI
â”‚ â””â”€â”€ private_ai_ui/
â”œâ”€â”€ model/ # (ignored) local LLM models
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

> âš ï¸ Large model files and virtual environments are excluded from GitHub using `.gitignore`.

---

## âš™ï¸ Requirements

### Backend
- Python 3.10+
- Ollama installed
- FastAPI
- Uvicorn

### Frontend
- Flutter SDK (stable)
- Chrome browser (for Flutter Web)

---

## ğŸ”§ Backend Setup

```bash
cd backend
python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn
