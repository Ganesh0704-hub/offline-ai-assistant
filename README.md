# Offline AI Assistant ü§ñ

An **offline AI assistant** built using **FastAPI**, **Ollama (local LLM)**, and **Flutter Web**.  
This project allows users to chat with an AI model **completely offline**, without relying on any cloud APIs.

---

## üöÄ Features

- ‚úÖ Fully **offline AI chat**
- ‚úÖ Uses **Ollama** for local LLM inference
- ‚úÖ **FastAPI** backend for API handling
- ‚úÖ **Flutter Web** frontend (works without OpenGL)
- ‚úÖ Simple REST API (`/chat`)
- ‚úÖ Clean and modular architecture
- ‚úÖ Cross-platform (runs on Linux, browser-based UI)

---

## üèóÔ∏è Architecture

Browser (Flutter Web UI)
|
| HTTP POST
v
FastAPI Backend (localhost:8000)
|
v
Ollama (Local LLM - TinyLlama)

## üìÅ Project Structure
offline_ai_assistant/
‚îú‚îÄ‚îÄ backend/ # FastAPI backend
‚îÇ ‚îú‚îÄ‚îÄ main.py
‚îÇ ‚îî‚îÄ‚îÄ venv/
‚îú‚îÄ‚îÄ frontend/ # Flutter Web UI
‚îÇ ‚îî‚îÄ‚îÄ private_ai_ui/
‚îú‚îÄ‚îÄ model/ # (ignored) local LLM models
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md

> ‚ö†Ô∏è Large model files and virtual environments are excluded from GitHub using `.gitignore`.

---

## ‚öôÔ∏è Requirements

### Backend
- Python 3.10+
- Ollama installed
- FastAPI
- Uvicorn

### Frontend
- Flutter SDK (stable)
- Chrome browser (for Flutter Web)

---

## üîß Backend Setup

```bash
cd backend
python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn
ollama run tinyllama
uvicorn main:app --host 127.0.0.1 --port 8000


## Frontend Setup (Flutter Web)
flutter config --enable-web
cd frontend/private_ai_ui
flutter run -d chrome
flutter run -d web-server --web-port 3000
http://localhost:3000


